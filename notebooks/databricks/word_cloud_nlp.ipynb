{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import AWS credentials\n",
    "# import config.py ##for local\n",
    "%run \"/dbfs/FileStore/tables/config\" ##for databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create Spark session\n",
    "app_name = \"spark-airbnb-sentiment\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "secret_name = my_secret_name\n",
    "region_name = my_region_name\n",
    "access_key  = my_access_key\n",
    "secret_key  = my_secret_key\n",
    "\n",
    "session      = boto3.session.Session(aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=region_name)\n",
    "client       = session.client('secretsmanager')\n",
    "secret_value = client.get_secret_value(SecretId=secret_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_connection(secret_value):\n",
    "    return json.loads(secret_value['SecretString'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = get_connection(secret_value)\n",
    "\n",
    "# Postgres credentials\n",
    "jdbcHostname = connection['host']\n",
    "jdbcPort     = connection['port']\n",
    "jdbcDatabase = \"postgres\"\n",
    "dialect      = \"postgresql\"\n",
    "jdbcUsername = connection['username']\n",
    "jdbcPassword = connection['password']\n",
    "\n",
    "jdbcUrl = f\"jdbc:{dialect}://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}\"\n",
    "connectionProperties = {\n",
    "  \"user\"     : jdbcUsername,\n",
    "  \"password\" : jdbcPassword,\n",
    "  \"driver\"   : \"org.postgresql.Driver\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from reviews_full table\n",
    "\n",
    "table = \"reviews_full\"\n",
    "\n",
    "reviews_df = spark.read.jdbc(url=jdbcUrl, table=table, properties=connectionProperties)\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "import sparknlp\n",
    "sparknlp.start()\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "pipeline = PretrainedPipeline(\"explain_document_ml\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.withColumnRenamed(\"comments\", \"text\").filter(\"text IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pipeline.transform(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(annotations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline for tokenized words in review\n",
    "stop_words_cleaner = StopWordsCleaner() \\\n",
    "        .setInputCols([\"token\"]) \\\n",
    "        .setOutputCol(\"cleanTokens\") \\\n",
    "        .setCaseSensitive(False) \\\n",
    "        .setStopWords([\"this\", \"is\", \"and\", \",\", \"!\", \".\", \"\\n\", \"\\t\", \"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\", \"(\", \")\", \"-\", \"/\", \";\"])\n",
    "\n",
    "unigrams = NGramGenerator() \\\n",
    "            .setInputCols([\"cleanTokens\"]) \\\n",
    "            .setOutputCol(\"unigrams\") \\\n",
    "            .setN(1) \\\n",
    "            .setEnableCumulative(False)\n",
    "\n",
    "bigrams = NGramGenerator() \\\n",
    "            .setInputCols([\"cleanTokens\"]) \\\n",
    "            .setOutputCol(\"bigrams\") \\\n",
    "            .setN(2) \\\n",
    "            .setEnableCumulative(False)\n",
    "\n",
    "extra_pipeline = Pipeline(\n",
    "    stages = [\n",
    "      stop_words_cleaner,\n",
    "      unigrams,\n",
    "      bigrams\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = extra_pipeline.fit(annotations_df)\n",
    "annotations_df = model.transform(annotations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(annotations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = spark.read.jdbc(url=jdbcUrl, table=\"listings_full\", properties=connectionProperties)\n",
    "listings_df = listings_df.select(\"id\", \"neighbourhood_cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to get zipcodes\n",
    "labeled_df = annotations_df.withColumnRenamed(\"id\", \"review_id\")\\\n",
    "                           .join(listings_df, annotations_df.listing_id == listings_df.id, how=\"inner\") \\\n",
    "                           .drop(F.col(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ngram results into list\n",
    "@F.udf()\n",
    "def extract_results(ngram):\n",
    "  combined_result = [x[\"result\"] for x in ngram ]\n",
    "  return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = labeled_df.select(\"listing_id\", \"neighbourhood_cleansed\", \\\n",
    "                                             extract_results(\"unigrams\").alias(\"unigram_list\"), \\\n",
    "                                             extract_results(\"bigrams\").alias(\"bigram_list\")\n",
    "                             ).withColumnRenamed(\"neighbourhood_cleansed\", \"zipcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by zipcode and create list of unigrams and bigrams\n",
    "agg_output_df = output_df.groupBy(\"zipcode\").agg(F.collect_list('unigram_list').alias(\"unigram_list\"), F.collect_list('bigram_list').alias(\"bigram_list\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error occurred here because the string array is a complex data type. Need to flatten it first to save to CSV\n",
    "display(agg_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten string arrays to strings\n",
    "def array_to_string(my_list):\n",
    "    return '[' + ','.join([str(elem) for elem in my_list]) + ']'\n",
    "\n",
    "array_to_string_udf = udf(array_to_string,StringType())\n",
    "\n",
    "final_agg_output_df = agg_output_df.withColumn('unigram_list_str',array_to_string_udf(F.col(\"unigram_list\"))).withColumn('bigram_list_str',array_to_string_udf(F.col(\"bigram_list\"))).drop(\"unigram_list\").drop(\"bigram_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display and save final output\n",
    "display(final_agg_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda [PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "sentiment_analysis",
  "notebookId": 2958204862315329
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
